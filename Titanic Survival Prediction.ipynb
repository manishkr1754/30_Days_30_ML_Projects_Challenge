{"metadata":{"kernelspec":{"display_name":"Python [conda env:Clustering]","language":"python","name":"conda-env-Clustering-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/manishkr1754/titanic-survival-prediction?scriptVersionId=144353920\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"---\n<center><h1>Titanic Survival Prediction</h1></center>\n<center><h3>Part of 30 Days 30 ML Projects Challenge</h3></center>\n\n---","metadata":{}},{"cell_type":"markdown","source":"## 1) Understanding Problem Statement\n---\n\nThe sinking of the Titanic remains one of the most tragic maritime disasters in history, prompting the need to understand and predict factors influencing survival. In the realm of data science, the Titanic dataset offers an opportunity to apply machine learning for survival prediction.\n\nThis project belongs to the domain of **Classification Machine Learning Problem**. The primary objective is **to build a predictive model that determines the likelihood of passengers surviving the Titanic disaster based on various features such as age, gender, class, and family relationships**. By leveraging these historical records, we aim to develop a model that can shed light on the factors contributing to survival and potentially improve the understanding of the tragic event while offering insights into passenger demographics and cabin arrangements.","metadata":{}},{"cell_type":"markdown","source":"## 2) Understanding Data\n---\n\nThe project uses **[Titanic Data](https://www.kaggle.com/competitions/titanic/data?select=train.csv)** which contains several variables (independent variables) and the outcome variable or dependent variable.\n\n[Data Description](https://www.kaggle.com/competitions/titanic/data)","metadata":{}},{"cell_type":"markdown","source":"## 3) Getting System Ready\n---\nImporting required libraries\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport warnings\nfrom six.moves import urllib\n\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\n# for model buidling\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4) Data Eyeballing\n---","metadata":{}},{"cell_type":"markdown","source":"### Laoding Data","metadata":{}},{"cell_type":"code","source":"titanic_data = pd.read_csv('Datasets/Day15_Titatnic_Data.csv') ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The size of Dataframe is: ', titanic_data.shape)\nprint('-'*100)\nprint('The Column Name, Record Count and Data Types are as follows: ')\ntitanic_data.info()\nprint('-'*100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining numerical & categorical columns\nnumeric_features = [feature for feature in titanic_data.columns if titanic_data[feature].dtype != 'O']\ncategorical_features = [feature for feature in titanic_data.columns if titanic_data[feature].dtype == 'O']\n\n# print columns\nprint('We have {} numerical features : {}'.format(len(numeric_features), numeric_features))\nprint('\\nWe have {} categorical features : {}'.format(len(categorical_features), categorical_features))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Missing Value Presence in different columns of DataFrame are as follows : ')\nprint('-'*100)\ntotal=titanic_data.isnull().sum().sort_values(ascending=False)\npercent=(titanic_data.isnull().sum()/titanic_data.isnull().count()*100).sort_values(ascending=False)\npd.concat([total, percent], axis=1, keys=['Total', 'Percent'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Summary Statistics of numerical features for DataFrame are as follows:')\nprint('-'*100)\ntitanic_data.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Summary Statistics of numerical features for DataFrame are as follows:')\nprint('-'*100)\ntitanic_data.describe(include='object')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_data['Survived'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5) Data Cleaning and Preprocessing\n---","metadata":{}},{"cell_type":"markdown","source":"### Handling Missing Values","metadata":{}},{"cell_type":"markdown","source":"#### Dropping the \"Cabin\" column from the dataframe","metadata":{}},{"cell_type":"code","source":"titanic_data = titanic_data.drop(columns='Cabin', axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Replacing the missing values in \"Age\" column with mean value","metadata":{}},{"cell_type":"code","source":"titanic_data['Age'].fillna(titanic_data['Age'].mean(), inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Replacing the missing values in \"Embarked\" column with mode value","metadata":{}},{"cell_type":"code","source":"titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Missing Value Presence in different columns of DataFrame are as follows : ')\nprint('-'*100)\ntotal=titanic_data.isnull().sum().sort_values(ascending=False)\npercent=(titanic_data.isnull().sum()/titanic_data.isnull().count()*100).sort_values(ascending=False)\npd.concat([total, percent], axis=1, keys=['Total', 'Percent'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Visualization","metadata":{}},{"cell_type":"markdown","source":"#### Survival Distribution","metadata":{}},{"cell_type":"code","source":"sns.countplot(x='Survived', data=titanic_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Gender Distribution","metadata":{}},{"cell_type":"code","source":"sns.countplot(x='Sex', data=titanic_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Genderwise Survival Distribution","metadata":{}},{"cell_type":"code","source":"sns.countplot(x='Sex', hue='Survived', data=titanic_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Passenger Class Distribution","metadata":{}},{"cell_type":"code","source":"sns.countplot(x='Pclass', data=titanic_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Passenger Classwise Survival Distribution","metadata":{}},{"cell_type":"code","source":"sns.countplot(x='Pclass', hue='Survived', data=titanic_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoding the Categorical Columns","metadata":{}},{"cell_type":"code","source":"titanic_data['Sex'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_data['Embarked'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting categorical Columns\ntitanic_data.replace({'Sex':{'male':0,'female':1}, 'Embarked':{'S':0,'C':1,'Q':2}}, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titanic_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6) Model Building\n---","metadata":{}},{"cell_type":"markdown","source":"### Creating Feature Matrix (Independent Variables) & Target Variable (Dependent Variable)","metadata":{}},{"cell_type":"code","source":"# separating the data and labels\nX = titanic_data.drop(columns = ['PassengerId','Name','Ticket','Survived'], axis=1) # Feature matrix\ny = titanic_data['Survived'] # Target variable","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Standardization","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler.fit(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"standardized_data = scaler.transform(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"standardized_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = standardized_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train-Test Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=45)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape, X_train.shape, X_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y.shape, y_train.shape, y_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Comparison : Training & Evaluation","metadata":{}},{"cell_type":"code","source":"models = [LogisticRegression, SVC, DecisionTreeClassifier, RandomForestClassifier]\naccuracy_scores = []\nprecision_scores = []\nrecall_scores = []\nf1_scores = []\n\nfor model in models:\n    classifier = model().fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    \n    accuracy_scores.append(accuracy_score(y_test, y_pred))\n    precision_scores.append(precision_score(y_test, y_pred))\n    recall_scores.append(recall_score(y_test, y_pred))\n    f1_scores.append(f1_score(y_test, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classification_metrics_df = pd.DataFrame({\n    \"Model\": [\"Logistic Regression\", \"SVM\", \"Decision Tree\", \"Random Forest\"],\n    \"Accuracy\": accuracy_scores,\n    \"Precision\": precision_scores,\n    \"Recall\": recall_scores,\n    \"F1 Score\": f1_scores\n})\n\nclassification_metrics_df.set_index('Model', inplace=True)\nclassification_metrics_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference\n\nThe comparison of machine learning models for Titanic survival prediction highlights some key insights. \n\n1. **Accuracy**: SVM achieves the highest accuracy at 85.47%, closely followed by Random Forest at 81.01%. This indicates that SVM has the best overall correct prediction rate, suggesting its effectiveness in classifying passengers as survivors or non-survivors.\n\n2. **Precision**: SVM also outperforms the other models in precision, with a score of 87.72%. This indicates that SVM is highly accurate when it predicts survival; it minimizes false positives, which is crucial in this context as it means fewer non-survivors being misclassified as survivors.\n\n3. **Recall**: SVM again demonstrates strength in recall, indicating its ability to identify a significant portion of actual survivors among all survivors in the dataset. It achieves a recall of 72.46%, which means it captures a substantial portion of survivors.\n\n4. **F1 Score**: SVM maintains its superiority in the F1 score, reflecting a balanced trade-off between precision and recall. It achieves an F1 score of 79.37%, which is indicative of its robust performance in predicting survival.\n\nIn summary, SVM emerges as the top-performing model in this context, excelling in accuracy, precision, recall, and F1 score. This suggests that SVM is a strong candidate for predicting Titanic survival based on the provided dataset, offering valuable insights into the factors influencing passenger survival rates.","metadata":{}}]}