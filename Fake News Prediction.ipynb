{"metadata":{"kernelspec":{"display_name":"Python [conda env:NLP39]","language":"python","name":"conda-env-NLP39-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/manishkr1754/fake-news-prediction?scriptVersionId=142780988\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"---\n<center><h1>Fake News Prediction</h1></center>\n<center><h3>Part of 30 Days 30 ML Projects Challenge</h3></center>\n\n---","metadata":{}},{"cell_type":"markdown","source":"## 1) Understanding Problem Statement\n---\n\n**Fake news** (intentionally false information) spread through traditional media and online social networks is causing harm in society. It is on the rise, leading to deception and division. To combat this, we need a dependable system that can tell real news from fake news. Such a system is essential to rebuild trust in media and protect the truth in our online and offline world.\n\nThe goal of this project is to employ machine learning techniques **to classify news articles as either genuine or fake based on their content and characteristics**. This **classification task** is fundamental in addressing the challenge of fake news and promoting information integrity and informed society. Moreover, It also involves use of **Natural Language Processing (NLP) techniques** for handling textual data. ","metadata":{}},{"cell_type":"markdown","source":"## 2) Understanding Data\n---\nIn this project, we work with a dataset referred to as **Fake News Data**. This dataset comprises various independent variables and one dependent variable for each individual news article.\n\n\n### Dataset Description:\n\nThe dataset consists of news articles and the goal of this project is to utilize machine learning techniques to predict the reliability of these articles based on their content and associated attributes.\n\nIt includes the following attributes:\n\n1. **id:** A unique identifier for each news article.\n2. **title:** The title of the news article.\n3. **author:** The author of the news article (if available).\n4. **text:** The content of the article, which may be incomplete.\n5. **label:** A categorical label indicating the potential reliability of the article:\n   - 1: Denotes articles that are potentially unreliable or fake.\n   - 0: Represents articles considered reliable or genuine.","metadata":{}},{"cell_type":"markdown","source":"## 3) Getting System Ready\n---\nImporting required libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# for text data preprocessing\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# for model buidling\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Downloading stop words for text preprocessing","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# printing the stopwords in English\nprint(stopwords.words('english'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4) Data Eyeballing\n---","metadata":{}},{"cell_type":"markdown","source":"### Laoding Data","metadata":{}},{"cell_type":"code","source":"fake_news_data = pd.read_csv('Datasets/Day4_Fake_News_Data.csv') ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_news_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The size of Dataframe is: ', fake_news_data.shape)\nprint('-'*100)\nprint('The Column Name, Record Count and Data Types are as follows: ')\nfake_news_data.info()\nprint('-'*100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining numerical & categorical columns\nnumeric_features = [feature for feature in fake_news_data.columns if fake_news_data[feature].dtype != 'O']\ncategorical_features = [feature for feature in fake_news_data.columns if fake_news_data[feature].dtype == 'O']\n\n# print columns\nprint('We have {} numerical features : {}'.format(len(numeric_features), numeric_features))\nprint('\\nWe have {} categorical features : {}'.format(len(categorical_features), categorical_features))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Missing Value Presence in different columns of DataFrame are as follows : ')\nprint('-'*100)\ntotal=fake_news_data.isnull().sum().sort_values(ascending=False)\npercent=(fake_news_data.isnull().sum()/fake_news_data.isnull().count()*100).sort_values(ascending=False)\npd.concat([total, percent], axis=1, keys=['Total', 'Percent'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_news_data['label'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5) Data Cleaning and Preprocessing\n---","metadata":{}},{"cell_type":"markdown","source":"### Replacing the null values with empty string","metadata":{}},{"cell_type":"code","source":"fake_news_data = fake_news_data.fillna('')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Merging the author name and news title","metadata":{}},{"cell_type":"code","source":"fake_news_data['content'] = fake_news_data['author']+' '+fake_news_data['title']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_news_data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking Missing Value Presence","metadata":{}},{"cell_type":"code","source":"print('Missing Value Presence in different columns of DataFrame are as follows : ')\nprint('-'*100)\ntotal=fake_news_data.isnull().sum().sort_values(ascending=False)\npercent=(fake_news_data.isnull().sum()/fake_news_data.isnull().count()*100).sort_values(ascending=False)\npd.concat([total, percent], axis=1, keys=['Total', 'Percent'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stemming\n\n- Stemming is the process of reducing a word to its Root word\n\n`For example:` actor, actress, acting --> act","metadata":{}},{"cell_type":"code","source":"porter_stemmer = PorterStemmer()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stemming(content):\n    stemmed_content = re.sub('[^a-zA-Z]',' ',content)\n    stemmed_content = stemmed_content.lower()\n    stemmed_content = stemmed_content.split()\n    stemmed_content = [porter_stemmer.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n    stemmed_content = ' '.join(stemmed_content)\n    return stemmed_content","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_news_data['content'] = fake_news_data['content'].apply(stemming)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_news_data['content']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6) Model Building\n---","metadata":{}},{"cell_type":"markdown","source":"### Creating Feature Matrix (Independent Variables) & Target Variable (Dependent Variable)","metadata":{}},{"cell_type":"code","source":"# separating the data and labels\nX = fake_news_data['content'] # Feature matrix\ny = fake_news_data['label'] # Target variable","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Converting the textual data to numerical data","metadata":{}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer()\nvectorizer.fit(X)\n\nX = vectorizer.transform(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train-Test Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=45)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape, X_train.shape, X_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y.shape, y_train.shape, y_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Comparison : Training & Evaluation","metadata":{}},{"cell_type":"code","source":"models = [LogisticRegression, SVC, DecisionTreeClassifier, RandomForestClassifier]\naccuracy_scores = []\nprecision_scores = []\nrecall_scores = []\nf1_scores = []\n\nfor model in models:\n    classifier = model().fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    \n    accuracy_scores.append(accuracy_score(y_test, y_pred))\n    precision_scores.append(precision_score(y_test, y_pred))\n    recall_scores.append(recall_score(y_test, y_pred))\n    f1_scores.append(f1_score(y_test, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classification_metrics_df = pd.DataFrame({\n    \"Model\": [\"Logistic Regression\", \"SVM\", \"Decision Tree\", \"Random Forest\"],\n    \"Accuracy\": accuracy_scores,\n    \"Precision\": precision_scores,\n    \"Recall\": recall_scores,\n    \"F1 Score\": f1_scores\n})\n\nclassification_metrics_df.set_index('Model', inplace=True)\nclassification_metrics_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference\n\n- All models (Logistic Regression, SVM, Decision Tree, and Random Forest) exhibit excellent performance in classifying fake news with high accuracy, precision, recall, and F1 score. Among them, **Random Forest** stands out as the top performer, providing a balanced approach to identifying fake news.\n\n**`Note:`** For real life best model selection are not solely based on accuracy score, we need to take into account other evaluation metrics, business context and model interpretability.","metadata":{}}],"kernelspec":{"display_name":"Python [conda env:NLP39]","language":"python","name":"conda-env-NLP39-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}}